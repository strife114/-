# 赛题1

## 基础环境搭建

1. 所有节点修改主机名并配置映射

   ```
   hostnamectl set-hostname master
   bash
   hostnamectl set-hostname node
   bash
   
   vim /etc/hosts
   10.18.4.14 master
   10.18.4.25 node
   ```

2. 关闭selinux和防火墙

   ```
   vim /etc/selinux/config
   SELINUX=disabled
   
   
   # systemctl stop firewalld.service
   # systemctl disable firewalld.service
   # yum remove -y NetworkManager firewalld
   # 清空防火墙规则
   yum install -y iptables-services
   iptables -F
   iptables -X
   iptables -Z
   service iptables save
   ```

3. 配置yum源

   master

   ```
   mount -o loop chinaskill_cloud_pass.iso /mnt/
   cp -rfv /mnt/* /opt/
   umount /mnt/
   
   rm -rf /etc/yum.repos.d/*
   vim /etc/yum.repos.d/local.repo
   [k8s]
   name=k8s
   baseurl=file:///opt/kubernetes-repo
   gpgcheck=0
   enabled=1
   
   # yum install -y vsftpd
   # vi /etc/vsftpd/vsftpd.conf 
   anon_root=/opt
   # systemctl start vsftpd && systemctl enable vsftpd
   ```

   node

   ```
   rm -rf /etc/yum.repos.d/*
   vim /etc/yum.repos.d/local.repo
   [k8s]
   name=k8s
   baseurl=ftp://master/kubernetes-repo
   gpgcheck=0
   enabled=1
   ```

   

## 部署Harbor仓库

1. 执行脚本

   ```
   cd /opt
   ./k8s_harbor_install.sh
   ```

2. 上传镜像

   ```
    cd /opt/
    ./k8s_image_push.sh
    输入镜像仓库地址(不加http/https): 10.18.4.10
   输入镜像仓库用户名: admin
   输入镜像仓库用户密码: Harbor12345
   您设置的仓库地址为: 10.24.2.32,用户名: admin,密码: xxx
   是否确认(Y/N): Y
   
   
   
   # 重启docker服务后，仓库不会重启，需要手动重启
   cd /opt/harbor/harbor
   ./prepare
   ./install.sh --with-clair
   ```



## 部署Kubernetes集群

1. 执行脚本

   ```
   cd /opt
   ./k8s_master_install.sh
   ```

2. node节点加入集群

   ```
   scp /opt/k8s_node_install.sh node:/opt/
   
   cd /opt
   ./k8s_node_install.sh
   ```

3. 在master上查看节点信息

   ```
   kubectl get nodes
   ```

   



# 基于kubernetes编排部署GPMall

## 容器化部署Redis

1. 编写Dockerfile

   ```
   # cd gpmall/
   # vim Dockerfile-redis
   FROM centos:centos7.5.1804
   MAINTAINER Guo
   
   # 配置yum源
   ADD gpmall.tar /opt
   RUN rm -rfv /etc/yum.repos.d/*
   ADD local.repo /etc/yum.repos.d/
   
   #安装Redis
   RUN yum -y install redis
   
   # 开放端口
   EXPOSE 6379
   
   #安装清理缓存文件
   RUN yum clean all
   
   #修改绑定IP地址
   RUN sed -i -e 's@bind 127.0.0.1@bind 0.0.0.0@g' /etc/redis.conf
   
   #关闭保护模式
   RUN sed -i -e 's@protected-mode yes@protected-mode no@g' /etc/redis.conf
   
   #启动
   ENTRYPOINT [ "/usr/bin/redis-server","/etc/redis.conf"]
   CMD []
   
   ```

2. 编写yum源文件

   ```
   # vim local.repo
   [gpmall]
   name=gpmall
   baseurl=file:///opt/gpmall
   gpgcheck=0
   enabled=1
   ```

3. 构建镜像

   ```
   docker build -t gpmall-redis:v1.0 -f Dockerfile-redis .
   ```



## 容器化部署Mariadb

1. 编写Dockerfile

   ```
   # vim Dockerfile-mariadb
   FROM centos:centos7.5.1804
   MAINTAINER Chinaskill
   
   # 配置yum源
   ADD gpmall.tar /opt
   RUN rm -rfv /etc/yum.repos.d/*
   ADD local.repo /etc/yum.repos.d/
   
   # 安装MariaDB
   RUN yum install -y MariaDB-server expect net-tools
   RUN yum clean all
   COPY gpmall.sql /opt/
   ADD mysql_init.sh /opt/
   RUN chmod +x /opt/mysql_init.sh
   RUN /opt/mysql_init.sh
   ENV LC_ALL en_US.UTF-8
   EXPOSE 3306
   CMD ["mysqld_safe"]
   
   ```

2. 编写初始化脚本

   ```
   # vim mysql_init.sh
   #!/bin/bash
   mysql_install_db --user=mysql
   (mysqld_safe &) | grep a
   sleep 3s
   mysqladmin -u root password '123456'
   sleep 3s
   mysql -uroot -p123456 -e "GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456'" 
   sleep 3s
   mysql -uroot -p123456 -e "create database gpmall;use gpmall;source /opt/gpmall.sql;"
   sleep 3s
   
   ```

3. 构建镜像

   ```
   docker build -t gpmall-mariadb:v1.0 -f Dockerfile-mariadb .
   ```





## 容器化部署ZooKeeper

1. 编写Dockerfile

   ```
   # vim Dockerfile-zookeeper
   FROM centos:centos7.5.1804
   MAINTAINER Chinaskill
   
   # 配置yum源
   ADD gpmall.tar /opt
   RUN rm -rfv /etc/yum.repos.d/*
   ADD local.repo /etc/yum.repos.d/
   
   # 安装JDK
   RUN yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel
   
   ENV work_path /usr/local
   
   WORKDIR $work_path
   
   # 安装ZooKeeper
   ADD zookeeper-3.4.14.tar.gz /usr/local
   ENV ZOOKEEPER_HOME /usr/local/zookeeper-3.4.14
   
   # PATH
   ENV PATH $PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$ZOOKEEPER_HOME/bin
   RUN cp $ZOOKEEPER_HOME/conf/zoo_sample.cfg $ZOOKEEPER_HOME/conf/zoo.cfg
   
   EXPOSE 2181
   
   # 设置开机自启
   CMD $ZOOKEEPER_HOME/bin/zkServer.sh start-foreground
   
   ```

2. 构建镜像

   ```
    docker build -t gpmall-zookeeper:v1.0 -f Dockerfile-zookeeper .
   ```



## 容器化部署Kafka

1. 编写Dockerfile

   ```
   # vim Dockerfile-kafka 
   FROM centos:centos7.5.1804
   MAINTAINER Chinaskill
   
   # 配置yum源
   ADD gpmall.tar /opt
   RUN rm -rfv /etc/yum.repos.d/*
   ADD local.repo /etc/yum.repos.d/
   
   # 安装JDK
   RUN yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel
   
   # 安装Kafka
   RUN mkdir /opt/kafka
   ADD kafka_2.11-1.1.1.tgz /opt/kafka
   RUN sed -i 's/num.partitions.*$/num.partitions=3/g' /opt/kafka/kafka_2.11-1.1.1/config/server.properties
   
   RUN echo "source /root/.bash_profile" > /opt/kafka/start.sh &&\
       echo "cd /opt/kafka/kafka_2.11-1.1.1" >> /opt/kafka/start.sh &&\
       echo "sed -i 's%zookeeper.connect=.*$%zookeeper.connect=zookeeper.mall:2181%g' /opt/kafka/kafka_2.11-1.1.1/config/server.properties" >> /opt/kafka/start.sh &&\
       echo "bin/kafka-server-start.sh config/server.properties" >> /opt/kafka/start.sh &&\
       chmod a+x /opt/kafka/start.sh
   
   EXPOSE 9092
   
   ENTRYPOINT ["sh", "/opt/kafka/start.sh"]
   
   ```

2. 构建镜像

   ```
   docker build -t gpmall-kafka:v1.0 -f Dockerfile-kafka .
   ```



## 容器化部署前端应用

1. 编写Dockerfile

   ```
   # vim Dockerfile-nginx
   FROM centos:centos7.5.1804
   MAINTAINER Chinaskill
   
   # 配置yum源
   ADD gpmall.tar /opt
   RUN rm -rfv /etc/yum.repos.d/*
   ADD local.repo /etc/yum.repos.d/
   
   RUN yum install -y cmake pcre pcre-devel openssl openssl-devel zlib-devel gcc gcc-c++ net-tools
   # 安装JDK
   RUN yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel
   
   RUN yum install nginx -y
   RUN rm -rf /usr/share/nginx/html/*
   ADD dist.tar /usr/share/nginx/html/
   COPY default.conf /etc/nginx/conf.d/
   COPY gpmall-shopping-0.0.1-SNAPSHOT.jar /root
   COPY gpmall-user-0.0.1-SNAPSHOT.jar /root
   COPY shopping-provider-0.0.1-SNAPSHOT.jar /root
   COPY user-provider-0.0.1-SNAPSHOT.jar /root
   COPY front-start.sh /root
   RUN chmod +x /root/front-start.sh
   
   EXPOSE 80 443
   CMD nginx -g "daemon off;"
   
   ```

2. 编写nginx配置文件

   ```
   # vim default.conf
   server {
       listen       80;
       server_name  localhost;
   
       #charset koi8-r;
       #access_log  /var/log/nginx/host.access.log  main;
   
       location / {
           root   /usr/share/nginx/html;
           index  index.html index.htm;
       }
       location /user {
               proxy_pass http://127.0.0.1:8082;
           }
   
       location /shopping {
               proxy_pass http://127.0.0.1:8081;
           }
   
       location /cashier {
               proxy_pass http://127.0.0.1:8083;
           }
       #error_page  404              /404.html;
   }
   
   ```

3. 编写启动脚本

   ```
   # vim front-start.sh
   #!/bin/bash
   nohup java -jar /root/user-provider-0.0.1-SNAPSHOT.jar &
   sleep 6
   nohup java -jar /root/shopping-provider-0.0.1-SNAPSHOT.jar &
   sleep 6
   nohup java -jar /root/gpmall-shopping-0.0.1-SNAPSHOT.jar &
   sleep 6
   nohup java -jar /root/gpmall-user-0.0.1-SNAPSHOT.jar &
   sleep 6
   nginx -g "daemon off;"
   
   ```

4. 构建镜像

   ```
   docker build -t gpmall-nginx:v1.0 -f Dockerfile-nginx .
   ```

   

## 编排部署GPMall

1. 将镜像推送到Harbor仓库

   ```
   for i in `docker images|grep gpmall|awk '{print$1":"$2}'`;do docker tag $i 192.168.223.200/library/$i;docker push 192.168.223.200/library/$i;done
   ```

2. 编写yaml文件

   ```
   # vim gpmall.yaml 
   apiVersion: v1
   kind: Pod
   metadata:
     name: chinaskill-mall
     labels:
       app: chinaskill-mall
   spec:
     containers:
     - name: chinaskill-mariadb
       image: 192.168.223.200/library/gpmall-mariadb:v1.0
       imagePullPolicy: IfNotPresent
       ports:
       - containerPort: 3306
   
     - name: chinaskill-redis
       image: 192.168.223.200/library/gpmall-redis:v1.0
       imagePullPolicy: IfNotPresent
       ports:
       - containerPort: 6379
   
     - name: chinaskill-zookeeper
       image: 192.168.223.200/library/gpmall-zookeeper:v1.0
       imagePullPolicy: IfNotPresent
       ports:
       - containerPort: 2181
   
     - name: chinaskill-kafka
       image: 192.168.223.200/library/gpmall-kafka:v1.0
       imagePullPolicy: IfNotPresent
       ports:
       - containerPort: 9092
   
     - name: chinaskill-nginx
       image: 192.168.223.200/library/gpmall-nginx:v1.0
       imagePullPolicy: IfNotPresent
       ports:
       - containerPort: 80
       - containerPort: 443
       command: ["/bin/bash","/root/front-start.sh"]
     hostAliases:
     - ip: "127.0.0.1"
       hostnames:
       - "mysql.mall"
       - "redis.mall"
       - "zookeeper.mall"
       - "kafka.mall"
   
   ---
   
   apiVersion: v1
   kind: Service
   metadata:
     name: chinaskill-mall
   spec:
     selector:
       app: chinaskill-mall
     ports:
     - port: 80
       targetPort: 80
       nodePort: 30080
     type: NodePort
   
   ```

3. 部署服务

   ```
   # kubectl apply -f gpmall.yaml
   pod/chinaskill-mall created
   service/chinaskill-mall created
   
   ```

4. 查看

   ```
   # kubectl get pods,service
   NAME                READY   STATUS    RESTARTS   AGE
   pod/chinaskill-mall      5/5        Running    0            40s
   
   NAME      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)     AGE
   service/chinaskill-mall   NodePort    10.106.85.20   <none>   80:30080/TCP   40s
   service/kubernetes      ClusterIP   10.96.0.1        <none>     443/TCP     57m
   
   ```

5. 验证

   ```
   浏览器访问
   本地ip:30080
   ```

   
