# 第一部分





## manila共享目录(!)

```bash
source /etc/keystone/admin-openrc.sh
manila type-create default_type_share False

manila type-list
manila type-show default_type_share

# 创建目录为2g的共享目录
manila create NFS 2 --name share1
manila list
manila show share1


# 挂载
# 开放share01目录对openstack管理网段使用权限
manila access-allow share1 ip 10.0.0.0/24 --access-level rw

manila access-list share1
# 查看挂载目录
manila show share1 | grep path | cut -d '|' -f3
mount -t nfs 172.30.17.5:/var/lib/manila/mnt/share-c3f5a9fc-a8e7-40a6-a43b-56cfd1738724 /mnt/




manila type-create default_type_share False
manila type-list
manila type-show default_type_share
manila create NFS 2 --name share1
manila list
manila show share1
manila access-allow share1 ip 10.0.0.0/24 --access-level rw
manila show share1 | grep path | cut -d '|' -f3
mount -t nfs 共享目录 /mnt


```

## cloudkitty计费(!)

```bash
# 启动hashmap
openstack rating modele enable hashmap


openstack rating hashmap service create service-flavor
openstack rating hashmap field create service-ID flavor_name
openstack rating hashmap mapping create --field-id fieldID -t flat --volume m1.small 1

openstack rating hashmap service create service-image
openstack rating hashmap mapping create -s serviceID -t flat 0.8

openstack rating hashmap service create dis_tests
openstack rating hashmap mapping create -s service-ID -t flat 0.8
# 设置dis_tests服务使用量超过10000G时提供8折优惠
openstack rating hashmap threshold create -s service-ID -t rate 10000 0.8

# 创建匹配规则
openstack rating hashmap service create volume.size
# 创建组
openstack rating hashmap group create  volume_thresholds
# 创建单价
openstack rating hashmap mapping create -s service id   -g group id  -t flat 0.01
# 创建阈值折扣
openstack rating hashmap threshold create -s service id -g gourp id -t rate 50 0.98



openstack rating hashmap service create service1
openstack rating hashmap field create serviceid flavor1
openstack rating hashmap mapping create --field-id fieldid -t flat --volume m1.small 1

openstack rating hashmap service create image
openstack rating hashmap mapping create -s serviceid -t flat 0.8

openstack rating hashmap service create dis_tests
openstack rating hashmap mapping create -s serviceid  -t flat 0.8
openstack rating hashmap threshold create -s serviceid -t rate 50 0.8

```

## barbican密钥(!)

```bash
# 密钥
openstack secret store --name 	secret01 --payload secretkey
openstack secret list
openstack secret show  secret01
openstack secret get [secret href]
openstack secret get [secret href] --payload
# 生成的密钥
openstack secret order create --name secret02 --algorithm aes --bit-length 256 --mode cbc --payload-content-type application/octet-stream key
openstack secret order list
openstack secret order get [secre href]





openstack secret store --name share01 --payload secretkey
openstack secret list
openstack secret get secrethref
openstack secret order create --name share02 --algorithm aes --bit-length 256 --mode cbc --payload-centent-type application/octet.stream key
```

## VPNaas隧道连接(!)

```shell
#admin租户创建路由网络
source /etc/keystone/admin-openrc.sh
#创建vxlan外网网络
openstack network create --external --share ext-net
openstack subnet create --subnet-range 100.0.0.0/24 --gateway 100.0.0.1 --network ext-net ext-subnet
#创建vxlan内网网络net1
openstack network create net1
openstack subnet create --subnet-range 100.0.1.0/24 --gateway 100.0.1.1 --network net1 net1
#创建路由route1，网关100.0.0.11，添加内网net1
openstack router create route1
openstack router set --external-gateway ext-net --fixed-ip subnet=ext-subnet,ip-address=100.0.0.11 route1
openstack router add subnet route1 net1


openstack network create net2
openstack subnet create --network net2 --subnet-range 100.0.2.0/24 --gateway 100.0.2.1 subnet-net
openstack router create route2
openstack router set --external-gateway ext-net --fixed-ip subnet=ext-subnet,ip-address=100.0.0.22 route2
openstack router add subnet route2 net2



openstack vpn ikd policy create ikepolicy1
openstack vpn ipsec policy create ipsecpolicy1
openstack vpn service create --router route1 --subnet net1 vpn1
openstakc vpn ipsec site connection create vpnconnectionabc --vpnservice vpn1 --ikepolicy ikepolicy1 --ipsecpolicy ipsecpolicy1 --peer-address 100.0.0.22 --peer-id 100.0.0.22 --peer-cidr 100.0.2.0/24 --pks secret 

openstack vpn ike policy create ikepolicy2
openstack vpn ipsec policy create ipsecpolicy2
openstack vpn service  create --router route2 --subnet net1 vpn2
openstack vpn ipsec site connection create vpnconnection --vpnservice vpn2 --ikspolicy ikspolicy2 --ipsecpolicy ipsecpolicy2 --peer-address 100.0.0.11 --peer-id 100.0.0.11 --peer-cidr 100.0.1.0/24 --pks secret




openstack network create --external --share wai-net
openstack subnet create --network wai-nei --subnet-range 100.0.0.0/24 --gateway 100.0.0.1 wai-subnet

openstack network create net1
openstack subnet create --network net1  --subnet-range 100.0.1.0 --gateway 100.0.1.1 subnet-net1
openstack network create net2
openstack subnet create --network net2 --subnet-range 100.0.2.0 --gateway 100.0.2.1 subnet-net2

openstack router create route1
openstack router set --external-gateway wai-net --fixed-ip subnet=wai-subnet,ip-address=100.0.0.100 route1
openstack router add subnet route1 net1
openstack router create route2
openstack router set --external-gateway wai-net --fixed-ip subnet=wai-subnet,ip-address=100.0.0.200 route2
openstack router add subnet route2 net2

openstack vpn ike policy create ikepolicy1
openstack vpn ipsec policy create ipsecpolicy1
openstack vpn service create --router route1  --subnet net1 vpn1
openstack vpn ipsec site connection create vpnconnection --vpnservice vpn1 --ikepolicy ikepolicy1 --ipsecpolicy ipsecpolicy1 --peer-address 100.0.0.200 --peer-id 100.0.0.200 --peer-cidr 100.0.2.0/24 --pks secret

openstack vpn ike policy create ikepolicy2
openstack vpn ipsec policy create ipsecpolicy2
openstack vpn service create --router route2 --subnet net2 vpn2
openstack vpn ipsec site connection create vpnconnection2 --ikepolicy ikepolicy2 --ipsecpolicy ipsecpolicy2 --peer-gateway 100.0.0.100 --peer-id 100.0.0.100 --peer-cidr 100.0.1.0/24 --pks secret
```



## NFS对接后端存储(!)

```shell
yum install -y nfs-utils rpcbind

vim /etc/exports
/mnt/test 192.168.200.0/24(rw,no_root_squash,no_all_squash,sync,anonuid=501,anongid=501)

# 生成配置
exportfs -r

shownmount -e 192.168.200.31

mount -t nfs 192.168.200.31:/mnt/test /var/lib/glance/images






```

## rabbitmq(!)

```shell
# 系统级别修改
vim /etc/sysctl.conf
# 在文件最下方添加fs.file-max=10240
fs.file-max=10240
sysctl -p

# 用户级别修改
vim /etc/security/limits.conf
openstack soft nofile 10240
openstack hard nofile 10240

vim /usr/lib/systemd/system/rabbitmq-server.service
# 在[service]下添加一行
LimitNOFILE=10240

systemctl daemon-reload
systemctl restart rabbitmq-server

# 查看rabbitmq最大连接数
rabbitmqctl status





vim /etc/sysctl.conf
fs.file-max=10240
vim /ete/security/limits.conf
openstack soft nofile 10240
openstack hard nofile 10240
vim /usr/lib/systemd/system/rabbitmq-server.service

[service]
LimitNOFILE=10240
systemctl daemon-reload
systemctl restart rabbitmq-server
rabbitmqctl status

```

## 开放镜像权限(!)

```shell
source /etc/keystone/admin-openrc.sh

openstack project list
openstack user list
glance image-create --name cirros --disk-format qcow2 --container-format bare < cirros----

glance member-create 镜像ID 项目ID

# 激活
glance member-update 镜像id xmid accepted





# 创建用户
openstack user create chinaskill --password 000000



openstack project list
opensatck user list
glance member-create 项目id 镜像id 
glance member-update 项目id 镜像id accepted
glance member-create 镜像id 项目id
glance member-update 镜像id 项目id accepted

openstack user create chinaskill --password 000000
```









# 第二部分

## 数据库调优(!)

```shell
vim /etc/my.cnf

#数据库支持大小写
lower_case_table_names =1
#数据库缓存
innodb_buffer_pool_size = 4G
#数据库的log buffer即redo日志缓冲
innodb_log_buffer_size = 64MB
#设置数据库的redo log即redo日志大小
innodb_log_file_size = 256MB
#数据库的redo log文件组即redo日志的个数配置
innodb_log_files_in_group = 2

systemctl restart mariadb

mysql -uroot -p000000 -e "show variables like 'innodb_log%';"


# 如果进不去就输入配置文件
skip-glant-tables





lower_case_table_names = 1
innodb_buffer_pool_size = 4G
innodb_log_buffer_size = 64MB
innodb_log_file_size = 256MB
innodb_log_files_in_group = 2
systemctl restart mariadb
mysql -uroot -p000000 -e "show variable like 'innodb_log%';"
```

## dashboard调优

```
# 根据题意找线索，关于django数据
#会发现有一行
#SESSION_ENGINE = 'django.contrib.sessions.backends.cache'存在cache里，改一改就行。

cat /etc/openstack-dashboard/local_settings | grep django 
SESSION_ENGINE = 'django.contrib.sessions.backends.file'

systemct restart httpd



django.con
systemctl restart httpd
django.con
systemctl restart httpd
```

## 句柄优化(!)

```
# 查看句柄数
ulimit -n

# 修改句柄数
echo "* soft nofile 65535" >> /etc/security/limits.conf
echo "* hard nofile 65535" >> /etc/security/limits.conf

# 生效
logout

ulimit -n

ulimit -n
vim /etc/security/limits.conf
* soft nofile 65535
* hard nofile 65535

logout
ulimit -n

```

## nova调度优化

```shell
# 解决因时间过长导致虚拟机启动超时从而获取不到ip地址而报错失败的问题
vim /etc/nova/nova.conf

# 去掉注释改为false
vif_plugging_is_fatal=false

# 重启服务
openstack-service restart
或
systemctl restart openstack-nova*



# 实体机重启后自动重启虚拟机
resume_guests_state_on_host_boot=true


service nova-compute restart



vif_plugging_is_fatal = false
resume_guests_state_on_host_boot=true
```

## nova清除缓存

```
vim /etc/nova/nova.conf
# 取消注释
remove_unused_base_images=true

openstack-service restart


remove_unused_base_images=true
```

​	

## 镜像优化(!)

```
# 查看镜像大小
du -sh 镜像名

# 镜像压缩
qemu-img convert -c -O qcow2 旧镜像名 新镜像名
-c 压缩
-O 输出格式


```

## I/O优化(!)

```
# 修改多队列调度策略
 cat /sys/block/vda/queue/scheduler
[mq-deadline] kyber none

echo none > /sys/block/vda/queue/scheduler


```

## 内存优化(!)

```
# 查看大页开启情况
grep Huge /proc/meminfo

# 修改大页数量
sysctl -w vm.nr_hugepages=20

# 启动透明大页
# echo always >/sys/kernel/mm/transparent_hugepage/enabled
# 启动内存共享
# echo always >/sys/kernel/mm/transparent_hugepage/defrag
# 关闭透明大页
# echo never >/sys/kernel/mm/transparent_hugepage/enabled
# 关闭内存共享
# echo never >/sys/kernel/mm/transparent_hugepage/defrag


sys/kernel/mm/transparent_hugepage
```

## SYN优化

```
# 系统修改
vim /etc/sysctl.conf
net.ipv4.tcp_syncookies = 1
sysctl -p










net.ipv4.tcp_syncookies =1
sysctl -p
```



## 脏数据回写

```
# 修改系统配置文件要求回写磁盘时间临时调整为60秒
vim /etc/sysctl.conf
vm.dirty_writeback_centisecs = 6000
sysctl -p



vm.dirty_writeback_centisecs = 6000
```



# 第三部分

## Nova关键参数调优

```
# vim /etc/nova/nova.conf
# 预留前2个物理CPU，把后面的所有CPU分配给虚拟机使用（假设vcpu为16个）
vcpu_pin_set=2-15

# 设置cpu超售比例为4倍
cpu_allocation_ratio=4.0
# 设置内存超售比例为1.5倍
ram_allocation_ratio=1.5

# 预留2048mb内存，这部分内存不能被虚拟机使用
reserved_host_memory_mb=2048
# 预留1024mb硬盘，这部分不能被使用
reserved_host_disk_mb=1024

# 设置nova服务心跳检查时间为120秒
service_down_time=120
# 设置超时时间
rpc_response_timeout=300 RPC
最大返回数据长度限制
osapi_max_limit = 5000





vcpu_pin_set=2-15
cpu_allocaiton-ratio=4.0
ram_allocation_ratio=1.5
reserved_host_memory_mb=2048
reserved_host_disk_mb=1024
rpc_response_timeout=300 RPC
service_down_time=120
osapi_max_limit=5000


```



## Glance关键参数调优

```
E vim /etc/glance/glance-api.conf
# 处理请求的子进程数量，如果为0则只有一个主进程
workers = 2 glance-api
# 最大返回数据长度限制
api_limit_max = 1000
# 一个响应中的最大返回项数
limit_param_default=1000




limit_param_default=1000

```















## glance对接cinder后端存储

```
# 将镜像存于cinder卷中，通过镜像使用cinder卷启动盘的方式创建虚拟机
vim /etc/cinder/cinder.conf
allowed_direct_url_schemes = cinder
image_upload_use_internal_tenant = true

vim /etc/glance/glance-api.conf
show_multiple_locations = true


glance image-create --name cirros-image --disk-format qcow2 --container-format bare --file cirros-0.3.4-x86 64-disk.img --progress 




```



## cinder限速

```
vim /etc/cinder/cinder.conf
# 找到[DEFAULT]部分并添加：
 replication_bandwidth_limit = 100MB
 
systemctl restart openstack-cinder*
systemctl restart openstack-cinder-volume.service


  

replication_bandwidth_limit=100mb
```





# 第四部分

## heat编排

### 命令

```
heat stack-create xystack -f server.yml
openstack stack list

heat_template_version: 2015-04-30

description: Simple template to deploy a single compute instance # 可选

resources:
  my_instance:
    type: OS::Nova::Server
    properties:
      key_name: my_key
      image: F18-x86_64-cfntools
      flavor: m1.small

resources 段是必须的，并且至少包含一个资源定义，上面就是定义了一个名为my_instance的资源

上面的例子会创建一个镜像为 F18-x86_64-cfntools, 大小为 m1.small, ssh key 指定为 my_key 的一个云主机


```



### 创建容器

```
heat_template_version: 2014-10-16
resources:
  swift:
    type: OS::Swift::Container
    properties:
      name: heat-swift
      
      
      
heat_template_version: 2000-1-1
resources:
  net-swift
    type: OS::Swift::Container
    properties:
      name: swift1
```

### 创建云主机

```
heat_template_version: 2014-10-16
resources:
  flavor:
    type: OS::Nova::Server
    properties:
      flavor: m1.small
      image: ubuntu-trusty-x86_64
      networks:
        - network: private
        
        
 heat_template_version: 2000-1-1
 resources:
   flavor:
     type: OS::Nova::Server
     properties:
       flavor:  m1.small
       image: cirros
       network:
         - network: net1
```

### 创建网络

```
heat_template_version: 2014-10-16
resources:
  new_net:
    type: OS::Neutron::Net

  new_subnet:
    type: OS::Neutron::Subnet
    properties:
      network_id: { get_resource: new_net }
      cidr: "10.8.1.0/24"
      dns_nameservers: [ "8.8.8.8", "8.8.4.4" ]
      ip_version: 4



```

### 创建云盘

```
resources:
  my_new_volume:
    type: OS::Cinder::Volume
    properties:
      size: 10

```













